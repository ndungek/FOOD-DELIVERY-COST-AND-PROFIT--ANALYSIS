{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<u>ONLINE FOOD DELIVERY PROFITABILITY ANALYSIS</u>**\n",
    "\n",
    "<span style=\"color: orange; font-style: italic;\">Maureen Ndunge Kitang'a</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PROJECT PROPOSAL**\n",
    "### *Executive Summary*\n",
    "\n",
    "Our analysis of Uber Eats data is geared towards extracting valuable insights into customer preferences within the food industry. We're looking at how much restaurants charge, their ratings, the types of food they offer, and where they're located. Our aim? Provide straightforward tips to help these places get noticed more on the platform and make more money.\n",
    "\n",
    "Analyzing this data helps us spot trends and patterns in the food scene. Our main goal is to provide practical recommendations that can reshape restaurant strategies and enhance their visibility on the Uber Eats platform.\n",
    "\n",
    "### *Problem Statement*\n",
    "Our main mission in this project is to provide practical guidance to clients planning to start a new restaurant chain or enhance the performance of their existing establishments. We're dealing with a widespread issue in the restaurant industry, where profit margins typically fall within the range of 10-20%. Our analysis is centered around exploring the current landscape of Uber Eats, aiming to uncover inventive strategies that can make restaurant businesses more attractive to both new and existing customers, ultimately driving higher profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WHY DOES THIS MATTER!?**\n",
    "The significance of this problem stems from the remarkable growth of online food ordering platforms, exemplified by Uber Eats' substantial transaction surge. In 2022, Uber Eats recorded a staggering USD 11 billion in revenue, marking a notable 31% increase from the previous year's revenue of $8.3 billion. Concurrently, there has been a steady 2% growth in user numbers, with a significant 10% increase in merchant participation in the US. These trends underscore the platform's rising popularity among both customers and merchants.\n",
    "\n",
    "Central to this challenge is understanding customer preferences across various dimensions, including preferred cuisines and menu diversity, within different regions. Thus, the pivotal question emerges: How can restaurants effectively analyze customer preferences to craft strategies that capitalize on the burgeoning potential of online food ordering platforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA UNDERSTANDING**\n",
    "The primary dataset utilized in this analysis has information on various restaurants spread across the United States. Data sources were obtained through web scraping collected using Python libraries and the Uber Eats website.\n",
    "There are 2 datasets - Restaurants dataset, and the Menus dataset. For more in information on the [data](\"https://www.kaggle.com/datasets/ahmedshahriarsakib/uber-eats-usa-restaurants-menus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 1:Restaurants Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data into a Pandas DataFrame\n",
    "restaurants = pd.read_csv(\"./data/restaurants.csv\")\n",
    "#Previewing the first few rows\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(restaurants.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(restaurants.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General information about the dataset\n",
    "restaurants.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The restaurants dataset 63469 rows and 11 columns. The columns with missing data are `score`, `ratings`, `category`, `price_range`, `full_address`,and `zip_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 2:Menus Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data into the Pandas DataFrame\n",
    "menus = pd.read_csv(\"./data/restaurant-menus.csv\")\n",
    "#Previewing the last few rows\n",
    "menus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(menus.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(menus.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General information about the dataset\n",
    "menus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA PREPARATION**\n",
    "\n",
    "During the Data Preparation phase, we will be performing a series of essential tasks to prepare our raw data for analysis. This phase includes the following key activities:\n",
    "\n",
    "*Merging Datasets:*\n",
    "We will combine multiple datasets, if available, to create a comprehensive dataset that encompasses all relevant information for our analysis. This may involve joining datasets based on common keys or merging them using appropriate techniques.\n",
    "\n",
    "*Deriving New Attributes:*\n",
    "To enhance the richness of our dataset and capture additional insights, we will create new attributes or features through feature engineering. This process involves transforming existing variables, generating new variables, or extracting valuable information from the data.\n",
    "\n",
    "*Data Cleaning:*\n",
    "Data cleaning is a crucial step that involves identifying and addressing various data quality issues, such as missing values, outliers, duplicates, and inconsistencies. We will employ techniques such as imputation, deletion, outlier detection, and data validation to ensure the integrity and quality of our dataset.\n",
    "\n",
    "*Exploratory Data Analysis (EDA):*\n",
    "EDA plays a vital role in understanding the underlying patterns, trends, and relationships within our data. We will perform exploratory data analysis to visualize distributions, examine correlations, detect patterns, and gain insights into the characteristics of our dataset. This will guide our subsequent analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA CLEANING**\n",
    ">> We will begin our Data Preparation phase by prioritizing data cleaning for individual datasets before merging. This approach allows us to address data inconsistencies, missing values, duplicates, outliers, and other quality issues specific to each dataset. By cleaning the datasets individually, we can ensure data integrity and consistency before merging. Additionally, considering differences in data sizes and structures among datasets, cleaning them separately facilitates more focused and efficient data cleaning efforts. Once each dataset is cleaned and standardized, we will proceed with merging them to create a comprehensive dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "   \n",
    "    missing_count = df.isnull().sum()  # Count missing values in each column\n",
    "    missing_percentage = (missing_count / len(df)) * 100  # Calculate percentage of missing values\n",
    "\n",
    "    # Create DataFrame to display missing values count and percentage\n",
    "    missing_values = pd.DataFrame({\n",
    "        'Missing Values': missing_count,\n",
    "        'Percentage': missing_percentage.round(2)\n",
    "    })\n",
    "    \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns '`id`, `position`, `name`, `lat`, and `lng` have no missing values (0% missing). These columns are complete and do not require imputation or further handling for missing data.\n",
    "\n",
    "The columns `score` and `ratings` have a significant proportion of missing values, with approximately `44.38%`missing in each column. This suggests that a large portion of the data in these columns is missing.Given the significance of these features and their importance for the analysis, median imputation will be done to handle the missing values.\n",
    "\n",
    "The columns `category`, `price_range`, `full_address`, and `zip_code` have relatively fewer missing values, ranging from 0.13% to 16.73%, it's worth considering whether these missing values are significant enough to warrant deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation for 'score' and 'ratings' columns\n",
    "median_score = restaurants['score'].median()\n",
    "median_ratings = restaurants['ratings'].median()\n",
    "restaurants['score'] = restaurants['score'].fillna(median_score)\n",
    "restaurants['ratings'] = restaurants['ratings'].fillna(median_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'category', 'price_range', 'full_address', and 'zip_code' columns\n",
    "restaurants.dropna(subset=['category', 'price_range', 'full_address', 'zip_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(menus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `description` column stands out as having a substantial number of missing values. To address this issue, rows with missing values in the 'description' column will be deleted to ensure data integrity.\n",
    "The other columns (`restaurant_id`, `category`, '`name`, and `price`) have either no missing values or a negligible number of missing values, suggesting that they are relatively complete and may not require extensive handling for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'description' column\n",
    "menus.dropna(subset=['description','name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> In addition to handling missing values, ensuring the absence of duplicate data is crucial for maintaining the integrity and reliability of our datasets. Data duplicates can skew analysis results, leading to inaccurate insights and conclusions. Therefore, as part of our data cleaning process, we will systematically check for and remove any duplicate entries within each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \n",
    "    duplicates = df.duplicated().any()\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(restaurants)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(menus)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Upon examining the menus DataFrame, it appears that duplicate rows have been detected. To further investigate the extent of duplication and gain insight into the duplicated data, we can inspect the DataFrame containing these duplicate rows. This will allow us to identify the specific duplicated entries and assess any patterns or inconsistencies present in the data. By closely examining these duplicates, we can make informed decisions regarding the appropriate actions to take, such as removal or additional data cleaning measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the 'menus' DataFrame across all columns\n",
    "duplicate_menus = menus[menus.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows for inspection\n",
    "print(\"Duplicates found in the DataFrame:\")\n",
    "print(duplicate_menus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the first occurrence of each duplicated row and drop the rest\n",
    "menus_cleaned = menus.drop_duplicates(keep='first')\n",
    "\n",
    "# Confirm that duplicates have been removed\n",
    "print(\"Duplicates removed. New DataFrame shape:\", menus_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Merging the Datasets*\n",
    ">> Merging datasets involves combining multiple datasets into a single comprehensive dataset. This process is essential for integrating data from different sources to perform unified analyses. In our case, we have two datasets: one containing information about restaurants and another containing menu data. Merging these datasets allows us to create a unified dataset that includes both restaurant details and their respective menus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the restaurant dataset with the menu dataset\n",
    "merged_data=restaurants.merge(menus, left_on='id', right_on='restaurant_id', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the merged dataset\n",
    "print(\"Shape of merged dataset:\", merged_data.shape)\n",
    "# Display the first few rows of the merged dataset in table format\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns listed below are being dropped during the data cleaning process for the following reasons:\n",
    "\n",
    "- `lat` and `lng`: These columns represent latitude and longitude coordinates, which are not relevant for the current analysis. However, for our current analysis, these coordinates are redundant as we have already extracted state and region information from the address column.\n",
    "- Since we have the full address and zip code, which provide sufficient location details for our analysis, keeping the latitude and longitude coordinates would be unnecessary.\n",
    "\n",
    "- `restaurant_id`: This column appears to be a duplicate of the `id` column, which likely serves as the unique identifier for each restaurant. Therefore, keeping both columns is unnecessary.\n",
    "\n",
    "- `description`: This column contains descriptions of food items from the menu dataset. Since the focus of the analysis is on restaurant characteristics and profitability, individual food item descriptions are not needed.\n",
    "\n",
    "- `name_y`: This column is likely a duplicate of the `name` column from one of the datasets. Keeping duplicate columns can lead to confusion and unnecessary redundancy in the data.\n",
    "\n",
    "- `category_y`: Similar to `name_y`, this column is likely a duplicate of the `category` column from one of the datasets. Removing duplicates helps maintain data consistency and clarity.\n",
    "\n",
    "- `price_range`: While the price range of items may be informative, the analysis might focus more on mean prices or specific item prices rather than general price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not relevant to the analysis\n",
    "cols_to_drop=['lat', 'lng', 'restaurant_id', 'description', 'name_y', 'category_y', 'price_range']\n",
    "merged_data = merged_data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removing Unwanted Characters*\n",
    "\n",
    ">> In this step, we aim to clean the dataset by removing unnecessary characters from specific columns. For instance, we remove the 'USD' currency symbol from the price column to ensure uniform representation and facilitate numerical analysis. By converting the prices to float type, we prepare the data for aggregation and computation. After calculating the mean prices for each restaurant, we merge the results back into the dataframe. Finally, we drop the redundant column, ensuring a cleaner and more standardized dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'USD' from the price of item column\n",
    "merged_data['price'] = merged_data['price'].str.strip(' USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prices into a float type\n",
    "merged_data['price'] = merged_data['price'].astype(float)\n",
    "\n",
    "# Grouping by restaurant id to find the mean prices for each restaurant\n",
    "mean_prices = merged_data.groupby('id')['price'].mean().reset_index()\n",
    "mean_prices['price'] = mean_prices['price'].round(2)\n",
    "\n",
    "# Merging mean prices back into dataframe\n",
    "merged_data = pd.merge(merged_data, mean_prices, on ='id', suffixes = ('','_mean'))\n",
    "\n",
    "# Renaming the price of item column to mean price per item\n",
    "merged_data.rename(columns = {'mean_price':'price'}, inplace = True)\n",
    "\n",
    "# Dropping the redundant price_of_item column\n",
    "merged_data.drop('price', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Extracting City and State from Full Address*\n",
    ">> We have extracted the city and state information from the \"full_address\" column in our DataFrame. Using a regular expression pattern, we located the city and state within the address string and created a new column called \"city_state\" to store this combined information. This allows us to isolate and analyze the geographical details of each location more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with all the states and their unique regions in the US\n",
    "state_to_region = {\n",
    "    'PA' : 'northeast_region',\n",
    "    'MD' : 'northeast_region',\n",
    "    'DE' : 'northeast_region',\n",
    "    'NJ' : 'northeast_region',\n",
    "    'NY' : 'northeast_region',\n",
    "    'CT' : 'northeast_region',\n",
    "    'RI' : 'northeast_region',\n",
    "    'MA' : 'northeast_region',\n",
    "    'NH' : 'northeast_region',\n",
    "    'VT' : 'northeast_region',\n",
    "    'ME' : 'northeast_region',\n",
    "    'FL' : 'southeast_region',\n",
    "    'GA' : 'southeast_region',\n",
    "    'AL' : 'southeast_region',\n",
    "    'MS' : 'southeast_region',\n",
    "    'LA' : 'southeast_region',\n",
    "    'AR' : 'southeast_region',\n",
    "    'TN' : 'southeast_region',\n",
    "    'KY' : 'southeast_region',\n",
    "    'WV' : 'southeast_region',\n",
    "    'VA' : 'southeast_region',\n",
    "    'NC' : 'southeast_region',\n",
    "    'SC' : 'southeast_region',\n",
    "    'TX' : 'southwest_region',\n",
    "    'OK' : 'southwest_region',\n",
    "    'NM' : 'southwest_region',\n",
    "    'AZ' : 'southwest_region',\n",
    "    'AK' : 'west_region',\n",
    "    'HI' : 'west_region',\n",
    "    'CA' : 'west_region',\n",
    "    'NV' : 'west_region',\n",
    "    'UT' : 'west_region',\n",
    "    'CO' : 'west_region',\n",
    "    'WY' : 'west_region',\n",
    "    'MT' : 'west_region',\n",
    "    'ID' : 'west_region',\n",
    "    'OR' : 'west_region',\n",
    "    'WA' : 'west_region',\n",
    "    'ND' : 'midwest_region',\n",
    "    'MN' : 'midwest_region',\n",
    "    'WI' : 'midwest_region',\n",
    "    'MI' : 'midwest_region',\n",
    "    'OH' : 'midwest_region',\n",
    "    'IN' : 'midwest_region',\n",
    "    'IL' : 'midwest_region',\n",
    "    'IA' : 'midwest_region',\n",
    "    'SD' : 'midwest_region',\n",
    "    'NE' : 'midwest_region',\n",
    "    'KS' : 'midwest_region',\n",
    "    'MO' : 'midwest_region',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<--Resources used:*\n",
    "\n",
    "ChatGPT was employed to assist in the extraction of state abbreviations from the address column and mapping them to their respective regions.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the full_address column to a string type\n",
    "merged_data['full_address'] = merged_data['full_address'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_state(address):\n",
    "    # Regular expression pattern to extract state abbreviations\n",
    "    state_pattern = r'\\b([A-Z]{2})\\b'\n",
    "    match = re.search(state_pattern, address)  \n",
    "    if match:\n",
    "        state = match.group(1)\n",
    "        if state in state_to_region:\n",
    "            return state\n",
    "    return None\n",
    "\n",
    "# Function to map states to regions\n",
    "def map_state_to_region(state):\n",
    "    return state_to_region.get(state, 'other_region')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *How Many States Are in The Dataset?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the extract_state function to create a 'state' and 'region' column\n",
    "merged_data['state'] = merged_data['full_address'].apply(extract_state)\n",
    "\n",
    "merged_data['region'] = merged_data['state'].apply(map_state_to_region)\n",
    "\n",
    "# Display the unique values in the state column\n",
    "print(merged_data['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the 25 states in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining only the resturants whose states are not Classified under None\n",
    "merged_data = merged_data[merged_data['state'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *What Is The Number Of Dishes Offered By Each Restaurant?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of dishes serve by the restaurant using groupby for restaurant ID\n",
    "menu = menus.groupby('restaurant_id')['name'].count()\n",
    "number_of_dishes_per_restaurant = pd.DataFrame(menu).reset_index()\n",
    "# Renaming the columns to id and number of dishes\n",
    "number_of_dishes_per_restaurant.columns = ['id', 'number_of_dishes']\n",
    "number_of_dishes_per_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the number_of_dishes_per_restaurant DataFrame with the main dataset merged_data\n",
    "merged_data = merged_data.merge(number_of_dishes_per_restaurant, on='id', how='left')\n",
    "\n",
    "# Drop duplicates after merging\n",
    "merged_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *How Many Restaurants In Each State?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by state to fint the number of resturants per state\n",
    "count_of_restaurants_by_state = merged_data.groupby('state').agg({'state':'count'})\n",
    "# Renaming the column to Number of Restaurants\n",
    "count_of_restaurants_by_state.columns = ['number_of_restaurants']\n",
    "# Sorting values in descending order\n",
    "count_of_restaurants_by_state.sort_values(by='number_of_restaurants', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Texas (TX)` has the highest number of restaurants: With `23,768` restaurants, Texas is the state with the highest number of restaurants in the dataset. This suggests that Texas has a vibrant and diverse restaurant industry.\n",
    "\n",
    "`Virginia (VA)` and `Washington (WA)` follow Texas: Virginia and Washington are the next two states with the highest numbers of restaurants, with `7,671` and `5,335` restaurants respectively. These states also have significant numbers of dining establishments.\n",
    "\n",
    "`Wisconsin (WI)` and `Utah (UT)` also have substantial restaurant numbers: Wisconsin and Utah have `3,837` and `2,528` restaurants respectively. This indicates that these states also have active dining scenes.\n",
    "\n",
    "Some states have relatively few restaurants: States like `Indiana (IN)`, `Hawaii (HI)`, and `Missouri (MO)` have only one restaurant each in the dataset. This could suggest a smaller or less developed restaurant industry in these states compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *How Many Cuisines Are Offered By Restaurants?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex pattern to match culture-related words\n",
    "culture_pattern = re.compile(r'\\b(?:American|Mexican|Italian|Greek|Chinese|Indian|Japanese|French|Spanish|Latin American|Mediterranean|Cuban|Caribbean|Middle Eastern|Cantonese|Asian Fusion|Asian|Black-owned|Soul Food)\\b', re.IGNORECASE)\n",
    "# Function to extract and replace culture in the 'category_of_restaurant' column\n",
    "def extract_and_replace_culture(row):\n",
    "    matches = culture_pattern.findall(str(row['category_x']))\n",
    "    return ', '.join(matches)\n",
    "# Apply the function to modify the 'category_of_restaurant' column in place\n",
    "restaurants['category_x'] = restaurants.apply(extract_and_replace_culture, axis=1)\n",
    "# We see that some columns have multiple cultures, we remove everything after the first comma in the 'category' column\n",
    "restaurants['category_x'] = restaurants['category_x'].str.split(',').str[0]\n",
    "# Replace blank values in 'category_of_restaurant' to null values\n",
    "restaurants.replace('', np.nan, inplace = True)\n",
    "# #Dropping null values from the 'category_of_restaurant' column\n",
    "restaurants.dropna(subset =['category_x'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the main cuisine from the category_x column\n",
    "def extract_main_cuisine(row):\n",
    "    categories = str(row['category_x']).split(',')\n",
    "    if categories:\n",
    "        return categories[0].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new column for the main cuisine\n",
    "merged_data['main_dish'] = merged_data.apply(extract_main_cuisine, axis=1)\n",
    "\n",
    "# Displaying the main cuisine for each restaurant\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column named `price_category` was introduced to categorize restaurants based on their price ranges, given the prevalence of missing values in the original 'price_range' column. The price range was calculated using 'price_of_item_mean,' excluding outliers and setting the lower bound as the minimum value and the upper bound 1.5 times the interquartile range above the upper quartile. Restaurants with prices falling within the first 1/3 of the price range were classified as 'cheap_prices,' those within the last 1/3 as 'high-end_prices,' and the remainder as 'average_prices.' This method allows for more precise restaurant categorization based on average item prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the Interquartile range\n",
    "iqr = (merged_data['price_mean'].quantile(q=0.75) - merged_data['price_mean'].quantile(q=0.25)) * 1.5\n",
    "\n",
    "# Divide the price of item mean range into 3 portions for categorizations\n",
    "# Sum up upper quartile and 1.5*interquartile range to exclude the extreme value and create upper bound.\n",
    "# Minus the upper bound by the minimum value and divided it by 3 to divide the range into 3 equal portions.\n",
    "interval = (merged_data['price_mean'].quantile(q=0.75) + iqr - merged_data['price_mean'].min()) / 3\n",
    "\n",
    "# low_p set the price at the first 1/3 of the range\n",
    "low_p = merged_data['price_mean'].min() + interval\n",
    "\n",
    "# high_p is set the price at 2/3 of the range\n",
    "high_p = low_p + interval\n",
    "\n",
    "# Create a list of our conditions. This was based on items that were less than or equal low_p, between low_p and upper_p percentile, and greater than or equal to the high_p percentile\n",
    "price_conditions = [\n",
    "    (merged_data['price_mean'] <= low_p),\n",
    "    (merged_data['price_mean'] > low_p) & (merged_data['price_mean'] < high_p),\n",
    "    (merged_data['price_mean'] >= high_p)\n",
    "]\n",
    "\n",
    "# Create a list of the values we want to assign for each condition\n",
    "price_values = ['cheap_prices', 'average_prices', 'high_end_prices']\n",
    "\n",
    "# Create a new column and use np.select to assign values to it using our lists as arguments\n",
    "merged_data['price_category'] = np.select(price_conditions, price_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> We are converting the data type of the `ratings` column from float to integer because ratings represent the number of ratings received by a restaurant. Since the number of ratings is always a whole number, it makes sense to store this data as integers rather than floats. This conversion ensures that the data accurately reflects the nature of ratings and allows for more efficient storage and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type of ratings column to integer\n",
    "merged_data['ratings'] = merged_data['ratings'].astype(int)\n",
    "\n",
    "# Rename ratings column to number_of_ratings\n",
    "merged_data.rename(columns={'ratings': 'number_of_ratings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXPLORATORY DATA ANALYSIS**\n",
    "\n",
    "Exploratory Data Analysis (EDA) plays a crucial role in our project, aiming to uncover insights and patterns within the Uber Eats dataset. We employ various types of analysis to understand the data better and inform decision-making for improving restaurant profitability on the platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Restaurant Location*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the world dataset from geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Filter the world dataset to include only the United States\n",
    "usa = world[world.name == 'United States']\n",
    "\n",
    "# Plot the map of the United States\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "usa.plot(ax=ax, color='lightgrey')\n",
    "\n",
    "# Plot restaurant locations on the map\n",
    "merged_data.plot(ax=ax, kind='scatter', x='lng', y='lat', color='red', alpha=0.5, s=5)\n",
    "plt.title('Restaurant Locations in the United States')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Restaurant Pricing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Filter out rows where price category is not equal to 0\n",
    "filtered_data = merged_data[merged_data['price_category'] != 0]\n",
    "# Define the order of price categories\n",
    "cat_order = ['cheap_prices', 'average_prices', 'high_end_prices']\n",
    "\n",
    "# Plotting the distribution of different price categories of restaurants in each region\n",
    "region_price = sns.catplot(x='price_category', \n",
    "                           data=filtered_data,  # Use filtered_data to exclude rows with price category 0\n",
    "                           kind='count', \n",
    "                           col='region', \n",
    "                           order=cat_order, \n",
    "                           hue='region',  # Add hue for region\n",
    "                           palette='Set2',  # Choose a color palette for regions\n",
    "                           legend=False)  # Disable legend for better layout\n",
    "\n",
    "# Set title and adjust layout\n",
    "region_price.fig.suptitle(\"Number of Price Category Restaurants in Each Region\", y=1.03)\n",
    "plt.legend(title='Region', loc='upper right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By segmenting restaurant pricing into three categories - cheap, average, and high-end - we gain insight into menu pricing disparities across regions. Across all regions, the majority of restaurants fall within the average price range. However, a closer examination of the quantity distribution among these categories reveals a notable trend in the southwest region, where the concentration is notably skewed towards the average price bracket. This nuanced understanding of market segmentation can inform strategic pricing decisions, empowering restaurants to differentiate themselves through competitive pricing strategies, such as offering below-average prices.\n",
    "\n",
    "Interestingly, the northeast region exhibits a higher proportion of high-end priced restaurants compared to cheaper options. This suggests a consumer base in this region willing to invest in premium dining experiences, even when ordering food for delivery.\n",
    "\n",
    "In summary, the visualization illustrates the distribution of various price categories across different regions in the USA:\n",
    "\n",
    "- **Cheap Prices Category:** Restaurants offering meals priced below or equal to $7.64.\n",
    "- **Average Prices Category:** Establishments with prices ranging between $7.64 - $14.29.\n",
    "- **High-End Prices Category:** Restaurants with prices above or equal to $14.29.\n",
    "\n",
    "The data underscores the dominance of restaurants falling within the 'Average Prices Category' across all regions, while also highlighting regional variations in consumer preferences and willingness to pay for food delivery services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'price_category' and calculating the mean number of dishes\n",
    "price_dishes = merged_data.groupby('price_category')['number_of_dishes'].mean().reset_index()\n",
    "print(price_dishes)\n",
    "print('')\n",
    "\n",
    "# Plotting the bar plot\n",
    "plt.figure(figsize=(8,8))\n",
    "cat_order = ['cheap_prices', 'average_prices', 'high_end_prices']\n",
    "sns.barplot(x='price_category', y='number_of_dishes', data=price_dishes, order=cat_order)\n",
    "plt.title('Average Number of Dishes by Price Category')\n",
    "plt.xlabel('Price Category')\n",
    "plt.ylabel('Average Number of Dishes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The barplot provides insights into the average number of dishes offered by restaurants across different price categories:\n",
    "\n",
    "1. **Average Prices Category**: Restaurants categorized under average prices offer an average of approximately 93.6 dishes. This suggests that establishments in this price range typically maintain a diverse menu selection to cater to a broad range of customer preferences. The relatively high number of dishes may indicate a focus on variety and affordability to attract a larger customer base.\n",
    "\n",
    "2. **Cheap Prices Category**: Restaurants in the cheap price category offer a slightly lower average number of dishes compared to the average price category, with approximately 93.0 dishes on average. Despite the lower pricing, these establishments still maintain a considerable menu variety, indicating a focus on providing value to customers while keeping operational costs low.\n",
    "\n",
    "3. **High-End Prices Category**: Restaurants categorized as high-end prices offer the highest average number of dishes, with approximately 97.8 dishes. This suggests that higher-priced restaurants prioritize offering extensive menu options, potentially focusing on gourmet or specialty items to justify their premium pricing. The larger menu selection may contribute to enhancing the overall dining experience and catering to discerning customers willing to pay premium prices for quality offerings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Restaurant Categories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Filtering the restaurants that are positioned 1st, 2nd, and 3rd, respectively on Uber Eats app\n",
    "r1=restaurants[restaurants['position']==1]\n",
    "r2=restaurants[restaurants['position']==2]\n",
    "r3=restaurants[restaurants['position']==3]\n",
    "\n",
    "#Defining a function to group dataset according to the categories and then plucking out the values & indexes.\n",
    "plt.style.use('ggplot')\n",
    "def func (R):\n",
    "  y1=R.groupby('category_of_restaurant')['name'].count().sort_values(ascending=False)\n",
    "  y2=y1.values\n",
    "  x2=y1.index\n",
    "  size=y2*5\n",
    "  return (x2,y2,size)\n",
    "vals1= func(r1)\n",
    "vals2= func(r2)\n",
    "vals3= func(r3)\n",
    "plt.scatter(x=vals1[0],y=vals1[1],s=vals1[2],label='Restaurants positioned 1',alpha=0.7)\n",
    "plt.scatter(x=vals2[0],y=vals2[1],s=vals2[2],label='Restaurants positioned 2',alpha=0.7)\n",
    "plt.scatter(x=vals3[0],y=vals3[1],s=vals3[2],label='Restaurants positioned 3',alpha=0.7)\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.ylim(0, 160)\n",
    "plt.xlabel(\"Category\",fontsize=13)\n",
    "plt.ylabel(\"Number of restaurants\",fontsize=13)\n",
    "plt.title(\"Relation between the number of restaurants and their positions on Uber Eats\",fontsize=10)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
