{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<u>ONLINE FOOD DELIVERY PROFITABILITY ANALYSIS</u>**\n",
    "\n",
    "<span style=\"color: orange; font-style: italic;\">Maureen Ndunge Kitang'a</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PROJECT PROPOSAL**\n",
    "### *Executive Summary*\n",
    "\n",
    "Our analysis of Uber Eats data is geared towards extracting valuable insights into customer preferences within the food industry. We're looking at how much restaurants charge, their ratings, the types of food they offer, and where they're located. Our aim? Provide straightforward tips to help these places get noticed more on the platform and make more money.\n",
    "\n",
    "Analyzing this data helps us spot trends and patterns in the food scene. Our main goal is to provide practical recommendations that can reshape restaurant strategies and enhance their visibility on the Uber Eats platform.\n",
    "\n",
    "### *Problem Statement*\n",
    "Our main mission in this project is to provide practical guidance to clients planning to start a new restaurant chain or enhance the performance of their existing establishments. We're dealing with a widespread issue in the restaurant industry, where profit margins typically fall within the range of 10-20%. Our analysis is centered around exploring the current landscape of Uber Eats, aiming to uncover inventive strategies that can make restaurant businesses more attractive to both new and existing customers, ultimately driving higher profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WHY DOES THIS MATTER!?**\n",
    "The significance of this problem stems from the remarkable growth of online food ordering platforms, exemplified by Uber Eats' substantial transaction surge. In 2022, Uber Eats recorded a staggering USD 11 billion in revenue, marking a notable 31% increase from the previous year's revenue of $8.3 billion. Concurrently, there has been a steady 2% growth in user numbers, with a significant 10% increase in merchant participation in the US. These trends underscore the platform's rising popularity among both customers and merchants.\n",
    "\n",
    "Central to this challenge is understanding customer preferences across various dimensions, including preferred cuisines and menu diversity, within different regions. Thus, the pivotal question emerges: How can restaurants effectively analyze customer preferences to craft strategies that capitalize on the burgeoning potential of online food ordering platforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA UNDERSTANDING**\n",
    "The primary dataset utilized in this analysis has information on various restaurants spread across the United States. Data sources were obtained through web scraping collected using Python libraries and the Uber Eats website.\n",
    "There are 2 datasets - Restaurants dataset, and the Menus dataset. For more in information on the [data](\"https://www.kaggle.com/datasets/ahmedshahriarsakib/uber-eats-usa-restaurants-menus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 1:Restaurants Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data into a Pandas DataFrame\n",
    "restaurants = pd.read_csv(\"./data/restaurants.csv\")\n",
    "#Previewing the first few rows\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(restaurants.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(restaurants.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General information about the dataset\n",
    "restaurants.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The restaurants dataset 63469 rows and 11 columns. The columns with missing data are `score`, `ratings`, `category`, `price_range`, `full_address`,and `zip_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 2:Menus Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data into the Pandas DataFrame\n",
    "menus = pd.read_csv(\"./data/restaurant-menus.csv\")\n",
    "#Previewing the last few rows\n",
    "menus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(menus.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(menus.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General information about the dataset\n",
    "menus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA PREPARATION**\n",
    "\n",
    "During the Data Preparation phase, we will be performing a series of essential tasks to prepare our raw data for analysis. This phase includes the following key activities:\n",
    "\n",
    "*Merging Datasets:*\n",
    "We will combine multiple datasets, if available, to create a comprehensive dataset that encompasses all relevant information for our analysis. This may involve joining datasets based on common keys or merging them using appropriate techniques.\n",
    "\n",
    "*Deriving New Attributes:*\n",
    "To enhance the richness of our dataset and capture additional insights, we will create new attributes or features through feature engineering. This process involves transforming existing variables, generating new variables, or extracting valuable information from the data.\n",
    "\n",
    "*Data Cleaning:*\n",
    "Data cleaning is a crucial step that involves identifying and addressing various data quality issues, such as missing values, outliers, duplicates, and inconsistencies. We will employ techniques such as imputation, deletion, outlier detection, and data validation to ensure the integrity and quality of our dataset.\n",
    "\n",
    "*Exploratory Data Analysis (EDA):*\n",
    "EDA plays a vital role in understanding the underlying patterns, trends, and relationships within our data. We will perform exploratory data analysis to visualize distributions, examine correlations, detect patterns, and gain insights into the characteristics of our dataset. This will guide our subsequent analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA CLEANING**\n",
    ">> We will begin our Data Preparation phase by prioritizing data cleaning for individual datasets before merging. This approach allows us to address data inconsistencies, missing values, duplicates, outliers, and other quality issues specific to each dataset. By cleaning the datasets individually, we can ensure data integrity and consistency before merging. Additionally, considering differences in data sizes and structures among datasets, cleaning them separately facilitates more focused and efficient data cleaning efforts. Once each dataset is cleaned and standardized, we will proceed with merging them to create a comprehensive dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "   \n",
    "    missing_count = df.isnull().sum()  # Count missing values in each column\n",
    "    missing_percentage = (missing_count / len(df)) * 100  # Calculate percentage of missing values\n",
    "\n",
    "    # Create DataFrame to display missing values count and percentage\n",
    "    missing_values = pd.DataFrame({\n",
    "        'Missing Values': missing_count,\n",
    "        'Percentage': missing_percentage.round(2)\n",
    "    })\n",
    "    \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns '`id`, `position`, `name`, `lat`, and `lng` have no missing values (0% missing). These columns are complete and do not require imputation or further handling for missing data.\n",
    "\n",
    "The columns `score` and `ratings` have a significant proportion of missing values, with approximately `44.38%`missing in each column. This suggests that a large portion of the data in these columns is missing.Given the significance of these features and their importance for the analysis, median imputation will be done to handle the missing values.\n",
    "\n",
    "The columns `category`, `price_range`, `full_address`, and `zip_code` have relatively fewer missing values, ranging from 0.13% to 16.73%, it's worth considering whether these missing values are significant enough to warrant deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation for 'score' and 'ratings' columns\n",
    "median_score = restaurants['score'].median()\n",
    "median_ratings = restaurants['ratings'].median()\n",
    "restaurants['score'] = restaurants['score'].fillna(median_score)\n",
    "restaurants['ratings'] = restaurants['ratings'].fillna(median_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'category', 'price_range', 'full_address', and 'zip_code' columns\n",
    "restaurants.dropna(subset=['category', 'price_range', 'full_address', 'zip_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_values(menus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `description` column stands out as having a substantial number of missing values. To address this issue, rows with missing values in the 'description' column will be deleted to ensure data integrity.\n",
    "The other columns (`restaurant_id`, `category`, '`name`, and `price`) have either no missing values or a negligible number of missing values, suggesting that they are relatively complete and may not require extensive handling for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'description' column\n",
    "menus.dropna(subset=['description','name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> In addition to handling missing values, ensuring the absence of duplicate data is crucial for maintaining the integrity and reliability of our datasets. Data duplicates can skew analysis results, leading to inaccurate insights and conclusions. Therefore, as part of our data cleaning process, we will systematically check for and remove any duplicate entries within each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \n",
    "    duplicates = df.duplicated().any()\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(restaurants)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(menus)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Upon examining the menus DataFrame, it appears that duplicate rows have been detected. To further investigate the extent of duplication and gain insight into the duplicated data, we can inspect the DataFrame containing these duplicate rows. This will allow us to identify the specific duplicated entries and assess any patterns or inconsistencies present in the data. By closely examining these duplicates, we can make informed decisions regarding the appropriate actions to take, such as removal or additional data cleaning measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the 'menus' DataFrame across all columns\n",
    "duplicate_menus = menus[menus.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows for inspection\n",
    "print(\"Duplicates found in the DataFrame:\")\n",
    "print(duplicate_menus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the first occurrence of each duplicated row and drop the rest\n",
    "menus_cleaned = menus.drop_duplicates(keep='first')\n",
    "\n",
    "# Confirm that duplicates have been removed\n",
    "print(\"Duplicates removed. New DataFrame shape:\", menus_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Merging the Datasets*\n",
    ">> Merging datasets involves combining multiple datasets into a single comprehensive dataset. This process is essential for integrating data from different sources to perform unified analyses. In our case, we have two datasets: one containing information about restaurants and another containing menu data. Merging these datasets allows us to create a unified dataset that includes both restaurant details and their respective menus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the restaurant dataset with the menu dataset\n",
    "merged_data=restaurants.merge(menus, left_on='id', right_on='restaurant_id', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the merged dataset\n",
    "print(\"Shape of merged dataset:\", merged_data.shape)\n",
    "# Display the first few rows of the merged dataset in table format\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns listed below are being dropped during the data cleaning process for the following reasons:\n",
    "\n",
    "- `lat` and `lng`: These columns represent latitude and longitude coordinates, which are not relevant for the current analysis. However, for our current analysis, these coordinates are redundant as we have already extracted state and region information from the address column.\n",
    "- Since we have the full address and zip code, which provide sufficient location details for our analysis, keeping the latitude and longitude coordinates would be unnecessary.\n",
    "\n",
    "- `restaurant_id`: This column appears to be a duplicate of the `id` column, which likely serves as the unique identifier for each restaurant. Therefore, keeping both columns is unnecessary.\n",
    "\n",
    "- `description`: This column contains descriptions of food items from the menu dataset. Since the focus of the analysis is on restaurant characteristics and profitability, individual food item descriptions are not needed.\n",
    "\n",
    "- `name_y`: This column is likely a duplicate of the `name` column from one of the datasets. Keeping duplicate columns can lead to confusion and unnecessary redundancy in the data.\n",
    "\n",
    "- `category_y`: Similar to `name_y`, this column is likely a duplicate of the `category` column from one of the datasets. Removing duplicates helps maintain data consistency and clarity.\n",
    "\n",
    "- `price_range`: While the price range of items may be informative, the analysis might focus more on mean prices or specific item prices rather than general price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not relevant to the analysis\n",
    "cols_to_drop=['lat', 'lng', 'restaurant_id', 'description', 'name_y', 'category_y', 'price_range']\n",
    "merged_data = merged_data.drop(cols_to_drop, axis=1)\n",
    "\"\"\"\n",
    "# Renaming the column names\n",
    "merged_data.columns=['id', 'position', 'name', 'score', 'ratings', 'category_of_restaurant', 'address', 'zip_code', 'price_of_item']\n",
    "# Renaming the 'ratings' column to reviews\n",
    "restaurants.rename(columns = {'ratings':'reviews'}, inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removing Unwanted Characters*\n",
    "\n",
    ">> In this step, we aim to clean the dataset by removing unnecessary characters from specific columns. For instance, we remove the 'USD' currency symbol from the price column to ensure uniform representation and facilitate numerical analysis. By converting the prices to float type, we prepare the data for aggregation and computation. After calculating the mean prices for each restaurant, we merge the results back into the dataframe. Finally, we drop the redundant column, ensuring a cleaner and more standardized dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'USD' from the price of item column\n",
    "merged_data['price'] = merged_data['price'].str.strip(' USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prices into a float type\n",
    "merged_data['price'] = merged_data['price'].astype(float)\n",
    "\n",
    "# Grouping by restaurant id to find the mean prices for each restaurant\n",
    "mean_prices = merged_data.groupby('id')['price'].mean().reset_index()\n",
    "mean_prices['price'] = mean_prices['price'].round(2)\n",
    "\n",
    "# Merging mean prices back into dataframe\n",
    "merged_data = pd.merge(merged_data, mean_prices, on ='id', suffixes = ('','_mean'))\n",
    "\n",
    "# Renaming the price of item column to mean price per item\n",
    "merged_data.rename(columns = {'mean_price':'price'}, inplace = True)\n",
    "\n",
    "# Dropping the redundant price_of_item column\n",
    "merged_data.drop('price', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Extracting City and State from Full Address*\n",
    ">> We have extracted the city and state information from the \"full_address\" column in our DataFrame. Using a regular expression pattern, we located the city and state within the address string and created a new column called \"city_state\" to store this combined information. This allows us to isolate and analyze the geographical details of each location more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the full_address column to a string type\n",
    "merged_data['full_address'] = merged_data['full_address'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def extract_city_state(address):\n",
    "    city_state_pattern = r'[A-Za-z\\s]+,\\s[A-Z]{2}'\n",
    "    match = re.search(city_state_pattern, address)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n",
    "\n",
    "# Apply the function to the full_address column in the DataFrame\n",
    "merged_data['city_state'] = merged_data['full_address'].apply(extract_city_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the full_address column\n",
    "merged_data.drop('full_address', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> We are converting the data type of the `ratings` column from float to integer because ratings represent the number of ratings received by a restaurant. Since the number of ratings is always a whole number, it makes sense to store this data as integers rather than floats. This conversion ensures that the data accurately reflects the nature of ratings and allows for more efficient storage and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type of ratings column to integer\n",
    "merged_data['ratings'] = merged_data['ratings'].astype(int)\n",
    "\n",
    "# Rename ratings column to number_of_ratings\n",
    "merged_data.rename(columns={'ratings': 'number_of_ratings'}, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
