{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<u>ONLINE FOOD DELIVERY PROFITABILITY ANALYSIS</u>**\n",
    "\n",
    "<span style=\"color: orange; font-style: italic;\">Maureen Ndunge Kitang'a</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PROJECT PROPOSAL**\n",
    "### *Executive Summary*\n",
    "\n",
    "Our analysis of Uber Eats data is geared towards extracting valuable insights into customer preferences within the food industry. We're looking at how much restaurants charge, their ratings, the types of food they offer, and where they're located. Our aim? Provide straightforward tips to help these places get noticed more on the platform and make more money.\n",
    "\n",
    "Analyzing this data helps us spot trends and patterns in the food scene. Our main goal is to provide practical recommendations that can reshape restaurant strategies and enhance their visibility on the Uber Eats platform.\n",
    "\n",
    "### *Problem Statement*\n",
    "Our main mission in this project is to provide practical guidance to clients planning to start a new restaurant chain or enhance the performance of their existing establishments. We're dealing with a widespread issue in the restaurant industry, where profit margins typically fall within the range of 10-20%. Our analysis is centered around exploring the current landscape of Uber Eats, aiming to uncover inventive strategies that can make restaurant businesses more attractive to both new and existing customers, ultimately driving higher profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WHY DOES THIS MATTER!?**\n",
    "The significance of this problem stems from the remarkable growth of online food ordering platforms, exemplified by Uber Eats' substantial transaction surge. In 2022, Uber Eats recorded a staggering USD 11 billion in revenue, marking a notable 31% increase from the previous year's revenue of $8.3 billion. Concurrently, there has been a steady 2% growth in user numbers, with a significant 10% increase in merchant participation in the US. These trends underscore the platform's rising popularity among both customers and merchants.\n",
    "\n",
    "Central to this challenge is understanding customer preferences across various dimensions, including preferred cuisines and menu diversity, within different regions. Thus, the pivotal question emerges: How can restaurants effectively analyze customer preferences to craft strategies that capitalize on the burgeoning potential of online food ordering platforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA UNDERSTANDING**\n",
    "The primary dataset utilized in this analysis has information on various restaurants spread across the United States. Data sources were obtained through web scraping collected using Python libraries and the Uber Eats website.\n",
    "There are 2 datasets - Restaurants dataset, and the Menus dataset. For more in information on the [data](\"https://www.kaggle.com/datasets/ahmedshahriarsakib/uber-eats-usa-restaurants-menus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 1:Restaurants Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>position</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>ratings</th>\n",
       "      <th>category</th>\n",
       "      <th>price_range</th>\n",
       "      <th>full_address</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>J' ti`'z Smoothie-N-Coffee Bar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coffee and Tea, Breakfast and Brunch, Bubble Tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1521 Pinson Valley Parkway, Birmingham, AL, 35217</td>\n",
       "      <td>35217</td>\n",
       "      <td>33.583640</td>\n",
       "      <td>-86.773330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Philly Fresh Cheesesteaks (541-B Graymont Ave)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American, Cheesesteak, Sandwiches, Alcohol</td>\n",
       "      <td>$</td>\n",
       "      <td>541-B Graymont Ave, Birmingham, AL, 35204</td>\n",
       "      <td>35204</td>\n",
       "      <td>33.509800</td>\n",
       "      <td>-86.854640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>Papa Murphy's (1580 Montgomery Highway)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pizza</td>\n",
       "      <td>$</td>\n",
       "      <td>1580 Montgomery Highway, Hoover, AL, 35226</td>\n",
       "      <td>35226</td>\n",
       "      <td>33.404439</td>\n",
       "      <td>-86.806614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>Nelson Brothers Cafe (17th St N)</td>\n",
       "      <td>4.7</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Breakfast and Brunch, Burgers, Sandwiches</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314 17th St N, Birmingham, AL, 35203</td>\n",
       "      <td>35203</td>\n",
       "      <td>33.514730</td>\n",
       "      <td>-86.811700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  position                                            name  score  \\\n",
       "0   1        19               PJ Fresh (224 Daniel Payne Drive)    NaN   \n",
       "1   2         9                  J' ti`'z Smoothie-N-Coffee Bar    NaN   \n",
       "2   3         6  Philly Fresh Cheesesteaks (541-B Graymont Ave)    NaN   \n",
       "3   4        17         Papa Murphy's (1580 Montgomery Highway)    NaN   \n",
       "4   5       162                Nelson Brothers Cafe (17th St N)    4.7   \n",
       "\n",
       "   ratings                                          category price_range  \\\n",
       "0      NaN                     Burgers, American, Sandwiches           $   \n",
       "1      NaN  Coffee and Tea, Breakfast and Brunch, Bubble Tea         NaN   \n",
       "2      NaN        American, Cheesesteak, Sandwiches, Alcohol           $   \n",
       "3      NaN                                             Pizza           $   \n",
       "4     22.0         Breakfast and Brunch, Burgers, Sandwiches         NaN   \n",
       "\n",
       "                                        full_address zip_code        lat  \\\n",
       "0      224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "1  1521 Pinson Valley Parkway, Birmingham, AL, 35217    35217  33.583640   \n",
       "2          541-B Graymont Ave, Birmingham, AL, 35204    35204  33.509800   \n",
       "3         1580 Montgomery Highway, Hoover, AL, 35226    35226  33.404439   \n",
       "4               314 17th St N, Birmingham, AL, 35203    35203  33.514730   \n",
       "\n",
       "         lng  \n",
       "0 -86.830703  \n",
       "1 -86.773330  \n",
       "2 -86.854640  \n",
       "3 -86.806614  \n",
       "4 -86.811700  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data into a Pandas DataFrame\n",
    "restaurants = pd.read_csv(\"./data/restaurants.csv\")\n",
    "#Previewing the first few rows\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows: 63469\n",
      "The number of columns:11\n"
     ]
    }
   ],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(restaurants.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(restaurants.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63469 entries, 0 to 63468\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   id            63469 non-null  int64  \n",
      " 1   position      63469 non-null  int64  \n",
      " 2   name          63469 non-null  object \n",
      " 3   score         35302 non-null  float64\n",
      " 4   ratings       35302 non-null  float64\n",
      " 5   category      63384 non-null  object \n",
      " 6   price_range   52852 non-null  object \n",
      " 7   full_address  63016 non-null  object \n",
      " 8   zip_code      62952 non-null  object \n",
      " 9   lat           63469 non-null  float64\n",
      " 10  lng           63469 non-null  float64\n",
      "dtypes: float64(4), int64(2), object(5)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#General information about the dataset\n",
    "restaurants.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The restaurants dataset 63469 rows and 11 columns. The columns with missing data are `score`, `ratings`, `category`, `price_range`, `full_address`,and `zip_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset 2:Menus Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5117212</th>\n",
       "      <td>63469</td>\n",
       "      <td>Other Essentials</td>\n",
       "      <td>Composition Notebook</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.38 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117213</th>\n",
       "      <td>63469</td>\n",
       "      <td>Other Essentials</td>\n",
       "      <td>Fancy Fest Savory Salmon - 3oz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.19 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117214</th>\n",
       "      <td>63469</td>\n",
       "      <td>Other Essentials</td>\n",
       "      <td>Bicycle Playing Cards</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.83 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117215</th>\n",
       "      <td>63469</td>\n",
       "      <td>Other Essentials</td>\n",
       "      <td>Tidy Cat Liter - 10lbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.38 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117216</th>\n",
       "      <td>63469</td>\n",
       "      <td>Other Essentials</td>\n",
       "      <td>7-Select Heavy Duty Foam Cooler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.8 USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         restaurant_id          category                             name  \\\n",
       "5117212          63469  Other Essentials             Composition Notebook   \n",
       "5117213          63469  Other Essentials   Fancy Fest Savory Salmon - 3oz   \n",
       "5117214          63469  Other Essentials            Bicycle Playing Cards   \n",
       "5117215          63469  Other Essentials           Tidy Cat Liter - 10lbs   \n",
       "5117216          63469  Other Essentials  7-Select Heavy Duty Foam Cooler   \n",
       "\n",
       "        description     price  \n",
       "5117212         NaN  4.38 USD  \n",
       "5117213         NaN  1.19 USD  \n",
       "5117214         NaN  3.83 USD  \n",
       "5117215         NaN  4.38 USD  \n",
       "5117216         NaN   6.8 USD  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data into the Pandas DataFrame\n",
    "menus = pd.read_csv(\"./data/restaurant-menus.csv\")\n",
    "#Previewing the last few rows\n",
    "menus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows: 5117217\n",
      "The number of columns:5\n"
     ]
    }
   ],
   "source": [
    "#Shape of the dataframe\n",
    "print(\"The number of rows: {}\".format(menus.shape[0]))\n",
    "\n",
    "print(\"The number of columns:{}\".format(menus.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5117217 entries, 0 to 5117216\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   restaurant_id  int64 \n",
      " 1   category       object\n",
      " 2   name           object\n",
      " 3   description    object\n",
      " 4   price          object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 195.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#General information about the dataset\n",
    "menus.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA PREPARATION**\n",
    "\n",
    "During the Data Preparation phase, we will be performing a series of essential tasks to prepare our raw data for analysis. This phase includes the following key activities:\n",
    "\n",
    "*Merging Datasets:*\n",
    "We will combine multiple datasets, if available, to create a comprehensive dataset that encompasses all relevant information for our analysis. This may involve joining datasets based on common keys or merging them using appropriate techniques.\n",
    "\n",
    "*Deriving New Attributes:*\n",
    "To enhance the richness of our dataset and capture additional insights, we will create new attributes or features through feature engineering. This process involves transforming existing variables, generating new variables, or extracting valuable information from the data.\n",
    "\n",
    "*Data Cleaning:*\n",
    "Data cleaning is a crucial step that involves identifying and addressing various data quality issues, such as missing values, outliers, duplicates, and inconsistencies. We will employ techniques such as imputation, deletion, outlier detection, and data validation to ensure the integrity and quality of our dataset.\n",
    "\n",
    "*Exploratory Data Analysis (EDA):*\n",
    "EDA plays a vital role in understanding the underlying patterns, trends, and relationships within our data. We will perform exploratory data analysis to visualize distributions, examine correlations, detect patterns, and gain insights into the characteristics of our dataset. This will guide our subsequent analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA CLEANING**\n",
    ">> We will begin our Data Preparation phase by prioritizing data cleaning for individual datasets before merging. This approach allows us to address data inconsistencies, missing values, duplicates, outliers, and other quality issues specific to each dataset. By cleaning the datasets individually, we can ensure data integrity and consistency before merging. Additionally, considering differences in data sizes and structures among datasets, cleaning them separately facilitates more focused and efficient data cleaning efforts. Once each dataset is cleaned and standardized, we will proceed with merging them to create a comprehensive dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "   \n",
    "    missing_count = df.isnull().sum()  # Count missing values in each column\n",
    "    missing_percentage = (missing_count / len(df)) * 100  # Calculate percentage of missing values\n",
    "\n",
    "    # Create DataFrame to display missing values count and percentage\n",
    "    missing_values = pd.DataFrame({\n",
    "        'Missing Values': missing_count,\n",
    "        'Percentage': missing_percentage.round(2)\n",
    "    })\n",
    "    \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>position</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>28167</td>\n",
       "      <td>44.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratings</th>\n",
       "      <td>28167</td>\n",
       "      <td>44.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>85</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_range</th>\n",
       "      <td>10617</td>\n",
       "      <td>16.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_address</th>\n",
       "      <td>453</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip_code</th>\n",
       "      <td>517</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lng</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Missing Values  Percentage\n",
       "id                         0        0.00\n",
       "position                   0        0.00\n",
       "name                       0        0.00\n",
       "score                  28167       44.38\n",
       "ratings                28167       44.38\n",
       "category                  85        0.13\n",
       "price_range            10617       16.73\n",
       "full_address             453        0.71\n",
       "zip_code                 517        0.81\n",
       "lat                        0        0.00\n",
       "lng                        0        0.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_missing_values(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns '`id`, `position`, `name`, `lat`, and `lng` have no missing values (0% missing). These columns are complete and do not require imputation or further handling for missing data.\n",
    "\n",
    "The columns `score` and `ratings` have a significant proportion of missing values, with approximately `44.38%`missing in each column. This suggests that a large portion of the data in these columns is missing.Given the significance of these features and their importance for the analysis, median imputation will be done to handle the missing values.\n",
    "\n",
    "The columns `category`, `price_range`, `full_address`, and `zip_code` have relatively fewer missing values, ranging from 0.13% to 16.73%, it's worth considering whether these missing values are significant enough to warrant deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation for 'score' and 'ratings' columns\n",
    "median_score = restaurants['score'].median()\n",
    "median_ratings = restaurants['ratings'].median()\n",
    "restaurants['score'] = restaurants['score'].fillna(median_score)\n",
    "restaurants['ratings'] = restaurants['ratings'].fillna(median_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'category', 'price_range', 'full_address', and 'zip_code' columns\n",
    "restaurants.dropna(subset=['category', 'price_range', 'full_address', 'zip_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>restaurant_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>1452145</td>\n",
       "      <td>28.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Missing Values  Percentage\n",
       "restaurant_id               0        0.00\n",
       "category                    0        0.00\n",
       "name                        4        0.00\n",
       "description           1452145       28.38\n",
       "price                       0        0.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_missing_values(menus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Dealing with Missing Values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `description` column stands out as having a substantial number of missing values. To address this issue, rows with missing values in the 'description' column will be deleted to ensure data integrity.\n",
    "The other columns (`restaurant_id`, `category`, '`name`, and `price`) have either no missing values or a negligible number of missing values, suggesting that they are relatively complete and may not require extensive handling for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of rows with missing values in 'description' column\n",
    "menus.dropna(subset=['description','name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> In addition to handling missing values, ensuring the absence of duplicate data is crucial for maintaining the integrity and reliability of our datasets. Data duplicates can skew analysis results, leading to inaccurate insights and conclusions. Therefore, as part of our data cleaning process, we will systematically check for and remove any duplicate entries within each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(df):\n",
    "    \n",
    "    duplicates = df.duplicated().any()\n",
    "    return duplicates\n",
    "\n",
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(restaurants)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the restaurants DataFrame\n",
    "has_duplicates = check_duplicates(menus)\n",
    "\n",
    "# Print the result\n",
    "if has_duplicates:\n",
    "    print(\"Duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Upon examining the menus DataFrame, it appears that duplicate rows have been detected. To further investigate the extent of duplication and gain insight into the duplicated data, we can inspect the DataFrame containing these duplicate rows. This will allow us to identify the specific duplicated entries and assess any patterns or inconsistencies present in the data. By closely examining these duplicates, we can make informed decisions regarding the appropriate actions to take, such as removal or additional data cleaning measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found in the DataFrame:\n",
      "         restaurant_id          category  \\\n",
      "379                  7  Nigiri / Sashimi   \n",
      "380                  7  Nigiri / Sashimi   \n",
      "381                  7  Nigiri / Sashimi   \n",
      "388                  7  Nigiri / Sashimi   \n",
      "10677              165         Beverages   \n",
      "...                ...               ...   \n",
      "5116337          63449   Spring & Summer   \n",
      "5116339          63449   Spring & Summer   \n",
      "5116341          63449   Spring & Summer   \n",
      "5117028          63469              Food   \n",
      "5117040          63469              Food   \n",
      "\n",
      "                                                      name  \\\n",
      "379                                            Tuna Tataki   \n",
      "380                                                 Amaebi   \n",
      "381                                                 Amaebi   \n",
      "388                                            Tuna Tataki   \n",
      "10677                                  Strawberry Lemonade   \n",
      "...                                                    ...   \n",
      "5116337  Moon & Stars Satin Trim Blanket And Animal Set...   \n",
      "5116339            Moon & Stars My Favorite Plush - 1.0 ea   \n",
      "5116341  Moon & Stars Satin Trim Blanket And Animal Set...   \n",
      "5117028                                 Mini Tacos - 10 pc   \n",
      "5117040                                 Mini Tacos - 10 pc   \n",
      "\n",
      "                                               description      price  \n",
      "379                                           Seared tuna.    6.0 USD  \n",
      "380                                          Sweet shrimp.    7.0 USD  \n",
      "381                                          Sweet shrimp.    7.0 USD  \n",
      "388                                           Seared tuna.    6.0 USD  \n",
      "10677    Chilled lemonade infused with the sweet taste ...   3.83 USD  \n",
      "...                                                    ...        ...  \n",
      "5116337  This baby item combines cute plush toy and nic...  15.73 USD  \n",
      "5116339  It is a warm & cozy cuddle pal that keeps your...  13.63 USD  \n",
      "5116341  This baby item combines cute plush toy and nic...  15.73 USD  \n",
      "5117028                                           10 Count   5.32 USD  \n",
      "5117040                                           10 Count   5.32 USD  \n",
      "\n",
      "[41420 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the 'menus' DataFrame across all columns\n",
    "duplicate_menus = menus[menus.duplicated(keep=False)]\n",
    "\n",
    "# Display duplicate rows for inspection\n",
    "print(\"Duplicates found in the DataFrame:\")\n",
    "print(duplicate_menus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. New DataFrame shape: (3644316, 5)\n"
     ]
    }
   ],
   "source": [
    "# Keep the first occurrence of each duplicated row and drop the rest\n",
    "menus_cleaned = menus.drop_duplicates(keep='first')\n",
    "\n",
    "# Confirm that duplicates have been removed\n",
    "print(\"Duplicates removed. New DataFrame shape:\", menus_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Merging the Datasets*\n",
    ">> Merging datasets involves combining multiple datasets into a single comprehensive dataset. This process is essential for integrating data from different sources to perform unified analyses. In our case, we have two datasets: one containing information about restaurants and another containing menu data. Merging these datasets allows us to create a unified dataset that includes both restaurant details and their respective menus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the restaurant dataset with the menu dataset\n",
    "merged_data=restaurants.merge(menus, left_on='id', right_on='restaurant_id', how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged dataset: (3666905, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>position</th>\n",
       "      <th>name_x</th>\n",
       "      <th>score</th>\n",
       "      <th>ratings</th>\n",
       "      <th>category_x</th>\n",
       "      <th>price_range</th>\n",
       "      <th>full_address</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>category_y</th>\n",
       "      <th>name_y</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>4.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Extra Large Pizza</td>\n",
       "      <td>Extra Large Meat Lovers</td>\n",
       "      <td>Whole pie.</td>\n",
       "      <td>15.99 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>4.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Extra Large Pizza</td>\n",
       "      <td>Extra Large Supreme</td>\n",
       "      <td>Whole pie.</td>\n",
       "      <td>15.99 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>4.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Extra Large Pizza</td>\n",
       "      <td>Extra Large Pepperoni</td>\n",
       "      <td>Whole pie.</td>\n",
       "      <td>14.99 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>4.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Extra Large Pizza</td>\n",
       "      <td>Extra Large BBQ Chicken &amp;amp; Bacon</td>\n",
       "      <td>Whole Pie</td>\n",
       "      <td>15.99 USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>PJ Fresh (224 Daniel Payne Drive)</td>\n",
       "      <td>4.6</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Burgers, American, Sandwiches</td>\n",
       "      <td>$</td>\n",
       "      <td>224 Daniel Payne Drive, Birmingham, AL, 35207</td>\n",
       "      <td>35207</td>\n",
       "      <td>33.562365</td>\n",
       "      <td>-86.830703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Extra Large Pizza</td>\n",
       "      <td>Extra Large 5 Cheese</td>\n",
       "      <td>Whole pie.</td>\n",
       "      <td>14.99 USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  position                             name_x  score  ratings  \\\n",
       "0  1.0      19.0  PJ Fresh (224 Daniel Payne Drive)    4.6     52.0   \n",
       "1  1.0      19.0  PJ Fresh (224 Daniel Payne Drive)    4.6     52.0   \n",
       "2  1.0      19.0  PJ Fresh (224 Daniel Payne Drive)    4.6     52.0   \n",
       "3  1.0      19.0  PJ Fresh (224 Daniel Payne Drive)    4.6     52.0   \n",
       "4  1.0      19.0  PJ Fresh (224 Daniel Payne Drive)    4.6     52.0   \n",
       "\n",
       "                      category_x price_range  \\\n",
       "0  Burgers, American, Sandwiches           $   \n",
       "1  Burgers, American, Sandwiches           $   \n",
       "2  Burgers, American, Sandwiches           $   \n",
       "3  Burgers, American, Sandwiches           $   \n",
       "4  Burgers, American, Sandwiches           $   \n",
       "\n",
       "                                    full_address zip_code        lat  \\\n",
       "0  224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "1  224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "2  224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "3  224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "4  224 Daniel Payne Drive, Birmingham, AL, 35207    35207  33.562365   \n",
       "\n",
       "         lng  restaurant_id         category_y  \\\n",
       "0 -86.830703            1.0  Extra Large Pizza   \n",
       "1 -86.830703            1.0  Extra Large Pizza   \n",
       "2 -86.830703            1.0  Extra Large Pizza   \n",
       "3 -86.830703            1.0  Extra Large Pizza   \n",
       "4 -86.830703            1.0  Extra Large Pizza   \n",
       "\n",
       "                                name_y description      price  \n",
       "0              Extra Large Meat Lovers  Whole pie.  15.99 USD  \n",
       "1                  Extra Large Supreme  Whole pie.  15.99 USD  \n",
       "2                Extra Large Pepperoni  Whole pie.  14.99 USD  \n",
       "3  Extra Large BBQ Chicken &amp; Bacon   Whole Pie  15.99 USD  \n",
       "4                 Extra Large 5 Cheese  Whole pie.  14.99 USD  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the merged dataset\n",
    "print(\"Shape of merged dataset:\", merged_data.shape)\n",
    "# Display the first few rows of the merged dataset in table format\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'position', 'name_x', 'score', 'ratings', 'category_x',\n",
      "       'price_range', 'full_address', 'zip_code', 'lat', 'lng',\n",
      "       'restaurant_id', 'category_y', 'name_y', 'description', 'price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns listed below are being dropped during the data cleaning process for the following reasons:\n",
    "\n",
    "- `lat` and `lng`: These columns represent latitude and longitude coordinates, which are not relevant for the current analysis. However, for our current analysis, these coordinates are redundant as we have already extracted state and region information from the address column.\n",
    "- Since we have the full address and zip code, which provide sufficient location details for our analysis, keeping the latitude and longitude coordinates would be unnecessary.\n",
    "\n",
    "- `restaurant_id`: This column appears to be a duplicate of the `id` column, which likely serves as the unique identifier for each restaurant. Therefore, keeping both columns is unnecessary.\n",
    "\n",
    "- `description`: This column contains descriptions of food items from the menu dataset. Since the focus of the analysis is on restaurant characteristics and profitability, individual food item descriptions are not needed.\n",
    "\n",
    "- `name_y`: This column is likely a duplicate of the `name` column from one of the datasets. Keeping duplicate columns can lead to confusion and unnecessary redundancy in the data.\n",
    "\n",
    "- `category_y`: Similar to `name_y`, this column is likely a duplicate of the `category` column from one of the datasets. Removing duplicates helps maintain data consistency and clarity.\n",
    "\n",
    "- `price_range`: While the price range of items may be informative, the analysis might focus more on mean prices or specific item prices rather than general price ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not relevant to the analysis\n",
    "cols_to_drop=['lat', 'lng', 'restaurant_id', 'description', 'name_y', 'category_y', 'price_range']\n",
    "merged_data = merged_data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removing Unwanted Characters*\n",
    "\n",
    ">> In this step, we aim to clean the dataset by removing unnecessary characters from specific columns. For instance, we remove the 'USD' currency symbol from the price column to ensure uniform representation and facilitate numerical analysis. By converting the prices to float type, we prepare the data for aggregation and computation. After calculating the mean prices for each restaurant, we merge the results back into the dataframe. Finally, we drop the redundant column, ensuring a cleaner and more standardized dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'USD' from the price of item column\n",
    "merged_data['price'] = merged_data['price'].str.strip(' USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prices into a float type\n",
    "merged_data['price'] = merged_data['price'].astype(float)\n",
    "\n",
    "# Grouping by restaurant id to find the mean prices for each restaurant\n",
    "mean_prices = merged_data.groupby('id')['price'].mean().reset_index()\n",
    "mean_prices['price'] = mean_prices['price'].round(2)\n",
    "\n",
    "# Merging mean prices back into dataframe\n",
    "merged_data = pd.merge(merged_data, mean_prices, on ='id', suffixes = ('','_mean'))\n",
    "\n",
    "# Renaming the price of item column to mean price per item\n",
    "merged_data.rename(columns = {'mean_price':'price'}, inplace = True)\n",
    "\n",
    "# Dropping the redundant price_of_item column\n",
    "merged_data.drop('price', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Extracting City and State from Full Address*\n",
    ">> We have extracted the city and state information from the \"full_address\" column in our DataFrame. Using a regular expression pattern, we located the city and state within the address string and created a new column called \"city_state\" to store this combined information. This allows us to isolate and analyze the geographical details of each location more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with all the states and their unique regions in the US\n",
    "\n",
    "state_to_region = {\n",
    "    'PA' : 'northeast_region',\n",
    "    'MD' : 'northeast_region',\n",
    "    'DE' : 'northeast_region',\n",
    "    'NJ' : 'northeast_region',\n",
    "    'NY' : 'northeast_region',\n",
    "    'CT' : 'northeast_region',\n",
    "    'RI' : 'northeast_region',\n",
    "    'MA' : 'northeast_region',\n",
    "    'NH' : 'northeast_region',\n",
    "    'VT' : 'northeast_region',\n",
    "    'ME' : 'northeast_region',\n",
    "    'FL' : 'southeast_region',\n",
    "    'GA' : 'southeast_region',\n",
    "    'AL' : 'southeast_region',\n",
    "    'MS' : 'southeast_region',\n",
    "    'LA' : 'southeast_region',\n",
    "    'AR' : 'southeast_region',\n",
    "    'TN' : 'southeast_region',\n",
    "    'KY' : 'southeast_region',\n",
    "    'WV' : 'southeast_region',\n",
    "    'VA' : 'southeast_region',\n",
    "    'NC' : 'southeast_region',\n",
    "    'SC' : 'southeast_region',\n",
    "    'TX' : 'southwest_region',\n",
    "    'OK' : 'southwest_region',\n",
    "    'NM' : 'southwest_region',\n",
    "    'AZ' : 'southwest_region',\n",
    "    'AK' : 'west_region',\n",
    "    'HI' : 'west_region',\n",
    "    'CA' : 'west_region',\n",
    "    'NV' : 'west_region',\n",
    "    'UT' : 'west_region',\n",
    "    'CO' : 'west_region',\n",
    "    'WY' : 'west_region',\n",
    "    'MT' : 'west_region',\n",
    "    'ID' : 'west_region',\n",
    "    'OR' : 'west_region',\n",
    "    'WA' : 'west_region',\n",
    "    'ND' : 'midwest_region',\n",
    "    'MN' : 'midwest_region',\n",
    "    'WI' : 'midwest_region',\n",
    "    'MI' : 'midwest_region',\n",
    "    'OH' : 'midwest_region',\n",
    "    'IN' : 'midwest_region',\n",
    "    'IL' : 'midwest_region',\n",
    "    'IA' : 'midwest_region',\n",
    "    'SD' : 'midwest_region',\n",
    "    'NE' : 'midwest_region',\n",
    "    'KS' : 'midwest_region',\n",
    "    'MO' : 'midwest_region',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the full_address column to a string type\n",
    "merged_data['full_address'] = merged_data['full_address'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_state(address):\n",
    "    # Regular expression pattern to extract state abbreviations\n",
    "    state_pattern = r'\\b([A-Z]{2})\\b'\n",
    "    match = re.search(state_pattern, address)  \n",
    "    if match:\n",
    "        state = match.group(1)\n",
    "        if state in state_to_region:\n",
    "            return state\n",
    "    return None\n",
    "\n",
    "# Function to map states to regions\n",
    "def map_state_to_region(state):\n",
    "    return state_to_region.get(state, 'other_region')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *How Many States Are in The Dataset?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL' None 'NE' 'WY' 'WI' 'MN' 'DE' 'IN' 'CT' 'IL' 'WV' 'WA' 'OH' 'OR'\n",
      " 'MT' 'ID' 'VA' 'MD' 'TN' 'VT' 'UT' 'TX' 'LA' 'AR' 'HI' 'MO']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the extract_state function to create a 'state' and 'region' column\n",
    "merged_data['state'] = merged_data['full_address'].apply(extract_state)\n",
    "\n",
    "merged_data['region'] = merged_data['state'].apply(map_state_to_region)\n",
    "\n",
    "# Display the unique values in the state column\n",
    "print(merged_data['state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the 25 states in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining only the resturants whose states are not Classified under None\n",
    "merged_data = merged_data[merged_data['state'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *What Is The Number Of Dishes Offered By Each Restaurant?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>number_of_dishes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60922</th>\n",
       "      <td>63465</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60923</th>\n",
       "      <td>63466</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60924</th>\n",
       "      <td>63467</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60925</th>\n",
       "      <td>63468</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60926</th>\n",
       "      <td>63469</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60927 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  number_of_dishes\n",
       "0          1                72\n",
       "1          2                46\n",
       "2          3                23\n",
       "3          4                36\n",
       "4          5                27\n",
       "...      ...               ...\n",
       "60922  63465               122\n",
       "60923  63466                 4\n",
       "60924  63467                 4\n",
       "60925  63468                34\n",
       "60926  63469                 4\n",
       "\n",
       "[60927 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of dishes serve by the restaurant using groupby for restaurant ID\n",
    "menu = menus.groupby('restaurant_id')['name'].count()\n",
    "number_of_dishes_per_restaurant = pd.DataFrame(menu).reset_index()\n",
    "# Renaming the columns to id and number of dishes\n",
    "number_of_dishes_per_restaurant.columns = ['id', 'number_of_dishes']\n",
    "number_of_dishes_per_restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> We are converting the data type of the `ratings` column from float to integer because ratings represent the number of ratings received by a restaurant. Since the number of ratings is always a whole number, it makes sense to store this data as integers rather than floats. This conversion ensures that the data accurately reflects the nature of ratings and allows for more efficient storage and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type of ratings column to integer\n",
    "merged_data['ratings'] = merged_data['ratings'].astype(int)\n",
    "\n",
    "# Rename ratings column to number_of_ratings\n",
    "merged_data.rename(columns={'ratings': 'number_of_ratings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXPLORATORY DATA ANALYSIS**\n",
    "\n",
    "Exploratory Data Analysis (EDA) plays a crucial role in our project, aiming to uncover insights and patterns within the Uber Eats dataset. We employ various types of analysis to understand the data better and inform decision-making for improving restaurant profitability on the platform.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
